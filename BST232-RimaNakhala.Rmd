---
title: "BST 232 Final Project"
author: "Rima Nakhala"
date: "12/11/2017"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(caret)
library(DAAG)
library(MASS)
library(mlbench)
library(tidyr)
library(corrgram)
library(corrplot)
library(class)
library(FSelector)
library(Metrics)
library(dplyr)
library(causaleffect)


setwd("/Users/rimanakhala/Docs/IE/R/_Machine Learning II/Final HW")

trainDF <-read.csv("train.csv",header=TRUE,sep=",",dec=".")
testDF <-read.csv("test.csv",header=TRUE,sep=",",dec=".")

```

## A neurotoxicity study on maternal exposure to metal pollutant in Bangladesh

A group of researchers is interested in investigating the effect of maternal exposure to manganese at high doses on child neuro-development in a sample of mother-infant pairs in Bangladesh. Bangladesh is experiencing an epidemic of metals and heavy metals poisoning. The soil of the country is contaminated with arsenic and manganese. Occupational and environmental exposure to lead is also a concern. The levels of arsenic, manganese, and lead found in Bangladesh can be extremely high and the support of these environmental toxicants poorly overlaps with what observed in Europe and in the USA.

In a sample of 400 mother infant pairs, the investigators measured child cognitive development (Y) at 24 months using the Bayley scales for infant neurodevelopment. Furthermore, they obtained a measure of concentrations of manganese (X1, mu/dl), the primary exposure of interest, as well as of arsenic (X2, mu/dl) and lead (X3, mu/dl) in cord blood. This measure reflects maternal exposure to these metals during pregnancy. An extensive questionnaire was filled at baseline by the mothers to collect information on potential confounders of the manganese-cognitive development relationship. These include child age at testing (Z1, months), mother education (Z2, years), mother age (Z3, centered at the mean), mother IQ (Z4, z-scored), home quality score (Z5), daily nutritional intake of egg (Z6), meat (Z7), fish (Z8), and maternal smoking status (Z9). The data is generated so that observations are be independent of each other and identically distributed (conditional on covariates). Under a certain model selection NUCA will hold. Note that not all covariates will be true confounders.

Link <https://www.kaggle.com/c/bst232>.

There are two aspects of the modeling that will be evaluated. 
1) One aspect is the predictive ability of the model 
2) and the other is the inferences about the causal effect of manganese on cognitive ability.

Data fields

     Y - child cognitive development score at 24 months (Bayley scales for infant neurodevelopment)
    X1 - concentrations of manganese (mu/dl)
    X2 - concentrations of arsenic in cord blood (mu/dl)
    X3 - concentrations of lead in cord blood (mu/dl)
    Z1 - mother age (months)
    Z2 - mother education (years)
    Z3 - mother age (centered at mean)
    Z4 - mother IQ (Z-score)
    Z5 - home quality score
    Z6 - daily nutritional intake of meat
    Z7 - daily nutritional intake of fish
    Z8 - daily nutritional intake of eggs
    Z9 - maternal smoking status

## Structure of the dataframe
```{r}

str(trainDF)

```

check if this data has missing values

```{r}
table(is.na(trainDF))
```

```{r}


colnames(trainDF) <- c("ID","con_manganese",
                         "con_arsenic",
                         "con_lead",
                         "child_age_month",
                         "mom_edu_year",
                         "mom_age_mean",
                         "mom_IQ",
                         "home_QS",
                         "daily_meat",
                         "daily_fish",
                         "daily_eggs",
                         "smoking",
                         "CCDS")

colnames(testDF) <- c("ID","con_manganese",
                         "con_arsenic",
                         "con_lead",
                         "child_age_month",
                         "mom_edu_year",
                         "mom_age_mean",
                         "mom_IQ",
                         "home_QS",
                         "daily_meat",
                         "daily_fish",
                         "daily_eggs",
                         "smoking")


summary(trainDF)

```
## Graphical Representation of Variables

We can see that majority of high child cognitive development score has been observed with low level of manganese concentrations which indicates negative correlation

```{r}
ggplot(trainDF, aes(x= CCDS, y = con_manganese)) + geom_point(size = 2.5, color="navy") + xlab("child cognitive development score") + ylab("Concentrations of Manganese)") + ggtitle("Concentrations of manganese by child's child cognitive development score")
```
let us also see correlations to prove it
```{r}
cor(trainDF$CCDS,trainDF$con_manganese)
```

We can see outliers in child cognitive development score which can distort predictions and affect the accuracy, and we should remove it 

```{r}

quantiles <- quantile(trainDF$CCDS, probs = c(.25, .75))
range <- 1.5 * IQR(trainDF$CCDS)
trainDf_subset <- subset(trainDF,
trainDF$CCDS > (quantiles[1] - range) & trainDF$CCDS < (quantiles[2] + range))


qplot(data = trainDf_subset, x = CCDS) + ylab("child cognitive development score")


```



## Remove Redundant Features
Calculate the correlation matrix of the numeric values in the dataset. Also correlated predictor variables brings down the model accuracy.
we can see high coorelation between mean age and age in month so mom age mean will be removed

```{r}
df.pearson <- cor(trainDf_subset[,1:12],use="pairwise.complete.obs",method="pearson")
corrplot(df.pearson, method="number")
```

```{r}

trainDf_subset <-  select(trainDf_subset,con_manganese,
                         con_arsenic,
                         con_lead,
                         child_age_month,
                         mom_edu_year,
                         mom_IQ,
                         home_QS,
                         daily_meat,
                         daily_fish,
                         daily_eggs,
                         smoking,
                         CCDS)

training_set <- trainDf_subset[1:200,]
test_set <- trainDf_subset[-(1:200),]



```


## Baseline Regression 

In order to set a baseline for the feature engineering process, fit a basic linear regression model to the dataset and evaluate its results in terms of accuracy.

Linear Regression takes following assumptions:

1-There exists a linear relationship between response and predictor variables
2-The predictor (independent) variables are not correlated with each other. Presence of collinearity leads to multicollinearity.
3-The error terms are uncorrelated. Otherwise, it will lead to autocorrelation.
4-Error terms must have constant variance. Non-constant variance leads to heteroskedasticity.

Adjusted R² measures the goodness of fit of a regression model. Higher the R², better is the model. Our R² = 0.9404

```{r}

lm_model <- lm(CCDS ~ ., data=training_set)

summary(lm_model)

```
Let’s check out regression plot to find out more ways to improve this model

```{r}
par(mfrow=c(2,2))
plot(lm_model)
```

## Recursive Feature Selection

We define the control using a random forest selection function with Cross-Validated (20 fold) and 15 repeats, to see top5 features

```{r message=FALSE, warning=FALSE}

# ensure the results are repeatable
set.seed(7)

# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=20, repeats = 15)
# run the RFE algorithm
results <- rfe(training_set[,1:11], training_set[,12], sizes=c(1:11), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```


## Feature Importance

As well, compute the Chi Squared test to measure the importance of the variables with respect to the target.

```{r}

fi <- chi.squared(CCDS ~ ., training_set)
fi$feature_name <- rownames(fi)
fi <- fi[order(- fi$attr_importance),]
print(fi)
```

Aslo conisder Information gain

```{r}
ig <- information.gain(CCDS ~ ., training_set, unit = "log2")
# Sort by importance
ig$feature_name <- rownames(ig)
ig <- ig[order(- ig$attr_importance),]

print(ig)
```

# Remove less important feautres

```{r}

tr_select <-  select(training_set,
                         con_manganese,
                         mom_edu_year,
                         home_QS,
                         daily_fish,
                         child_age_month,
                         CCDS)
cor(tr_select)
```

# Fit a second model

fit a 2nd linear regression model with the important features based on wrappre methods, now our R² = 0.9326 which is slightly less than the baseline model
```{r}
lm_model2 <- lm(CCDS ~ ., data=tr_select)
summary(lm_model2)
```
Plot model residuals

```{r}
par(mfrow=c(2,2))
plot(lm_model2)
```


However, we try to predict with the test set to see how the 2nd model scores in term of RMSE

```{r}
predicted <- predict(lm_model2, newdata =test_set, type = "response")
RMSE.lin.reg <- rmse(test_set$CCDS, predicted)
RMSE.lin.reg
```

## Another model with cross validation
fit a linear model with same selected features but with 40-fold CV which is repeated 10 times

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 40,
                           repeats = 10)

set.seed(825)
lm_model3 <- train(CCDS ~ ., data = tr_select, 
                 method = "lm", 
                 trControl = fitControl)
lm_model3
```
Predict again to see if the model is improved, although it shows in train set the RMSE score has improved to 1.19 the result with the test set showing the same score 1.35 

```{r}

predictedCV <- predict(lm_model3, newdata =test_set, type = "raw", trControl = fitControl)
newRMSE <- rmse(test_set$CCDS, predictedCV)

print(newRMSE)
```

## Preditct CCDS for the original test dataframe and save file for upload
```{r}
results_prob <- predict(lm_model3, newdata = testDF, type = "raw")
testDF$CCDS <- results_prob
sub_file <- data.frame(ID = testDF$ID, Y = testDF$CCDS)
write.csv(sub_file, 'uploadRN.csv')
```


